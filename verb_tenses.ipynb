{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import nltk\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from numpy import array as np_array\n",
    "from nltk.data import load\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "\n",
    "TENSED = True\n",
    "\n",
    "TENSED_POS_TAGS = ['VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n",
    "STANCES = ['FAVOR', 'NONE', 'AGAINST']\n",
    "# TAG_SET = {} \n",
    "tagdict = load('help/tagsets/upenn_tagset.pickle')\n",
    "TAG_SET = list(tagdict.keys())\n",
    "TAG_SET.append('#')\n",
    "\n",
    "\n",
    "def listOfTopics(dataframe):\n",
    "    listOfTopics = []\n",
    "    for row in traindf['Target']:\n",
    "        if row not in listOfTopics:\n",
    "            listOfTopics.append(row)\n",
    "    return listOfTopics\n",
    "\n",
    "\n",
    "def pos_tag(s):\n",
    "    return nltk.pos_tag(nltk.word_tokenize(s))\n",
    "\n",
    "\n",
    "def add_to_tag_set(s):\n",
    "    tags = nltk.pos_tag(nltk.word_tokenize(s['Tweet']))\n",
    "    for tag in tags:\n",
    "        if tag[1] in TAG_SET:\n",
    "            TAG_SET[tag[1]] = TAG_SET[tag[1]] + 1\n",
    "        else:\n",
    "            TAG_SET[tag[1]] = 1\n",
    "\n",
    "\n",
    "def get_tag_set(rows):\n",
    "    list(map(add_to_tag_set, rows))\n",
    "\n",
    "\n",
    "def tag_is_tensed(tag):\n",
    "    return tag[1] in TENSED_POS_TAGS\n",
    "\n",
    "\n",
    "def get_target_rows_in_frame(d_frame, target):\n",
    "    if target == 'ALL':\n",
    "        return list(\n",
    "            map((lambda x: x[1]), d_frame.iterrows()))\n",
    "    return list(\n",
    "        filter((lambda row: row['Target'] == target),\n",
    "            map((lambda x: x[1]), d_frame.iterrows())))\n",
    "\n",
    "\n",
    "def get_target_rows(filename, target):\n",
    "    training_df = pandas.read_csv(filename, sep='\\t', encoding='latin1')\n",
    "    return get_target_rows_in_frame(training_df, target)\n",
    "\n",
    "\n",
    "def get_tagged_words(rows):\n",
    "    return list( map((lambda row: pos_tag(row['Tweet'])), rows) )\n",
    "\n",
    "\n",
    "def tag_counts(tags):\n",
    "    just_tags = list(map((lambda x: x[1]), tags))\n",
    "    return list(map((lambda tag: just_tags.count(tag)), TAG_SET))\n",
    "\n",
    "def tensed_tag_counts(tags):\n",
    "    just_tags = list(map((lambda x: x[1]), tags))\n",
    "    return list(map((lambda tag: just_tags.count(tag)), TENSED_POS_TAGS))\n",
    "\n",
    "def get_x_train(tags_for_tweets, tag_count_func):\n",
    "    return list(map(tag_count_func, tags_for_tweets))\n",
    "\n",
    "\n",
    "def onehot_for_stance(stance):\n",
    "    one_hot = [0,0,0]\n",
    "    one_hot[STANCES.index(stance)] = 1\n",
    "    return one_hot\n",
    "\n",
    "\n",
    "def get_y_train(rows):\n",
    "    return list(\n",
    "        map(onehot_for_stance,\n",
    "            map((lambda row: row['Stance']), rows)))\n",
    "\n",
    "\n",
    "def get_xy_data(filename, target, tensed=False):\n",
    "    target_rows = get_target_rows(filename, target)\n",
    "    # Get tags\n",
    "#     get_tag_set(target_rows)\n",
    "\n",
    "    tag_count_func = tag_counts\n",
    "    if tensed:\n",
    "        tag_count_func = tensed_tag_counts\n",
    "        \n",
    "    tagged_tweets = get_tagged_words(target_rows)\n",
    "    x_train = get_x_train(tagged_tweets, tag_count_func)\n",
    "    y_train = get_y_train(target_rows)\n",
    "    return (x_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(label, x_train, y_train, tensed=False):\n",
    "    # LEARN\n",
    "    input_dimen = len(TENSED_POS_TAGS) if tensed else len(TAG_SET)\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units=32, input_dim=input_dimen)) # layer\n",
    "    model.add(Activation('relu')) # layer\n",
    "    model.add(Dense(units=64))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(units=3)) # layer\n",
    "    model.add(Activation('softmax')) #layer\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=keras.optimizers.Adam(lr=0.001),\n",
    "                  metrics=['accuracy']) # not layer\n",
    "\n",
    "    history = model.fit(np_array(x_train), np_array(y_train), validation_split=0.33, epochs=100, batch_size=32, verbose=0)\n",
    "#     # summarize history for accuracy\n",
    "#     plt.plot(history.history['acc'])\n",
    "#     plt.plot(history.history['val_acc'])\n",
    "#     plt.title(label + ' model accuracy')\n",
    "#     plt.ylabel('accuracy')\n",
    "#     plt.xlabel('epoch')\n",
    "#     plt.legend(['train', 'test'], loc='upper left')\n",
    "#     plt.show()\n",
    "#     # summarize history for loss\n",
    "#     plt.plot(history.history['loss'])\n",
    "#     plt.plot(history.history['val_loss'])\n",
    "#     plt.title(label + ' model loss')\n",
    "#     plt.ylabel('loss')\n",
    "#     plt.xlabel('epoch')\n",
    "#     plt.legend(['train', 'test'], loc='upper left')\n",
    "#     plt.show()\n",
    "    # Return our model yo\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\n",
    "    'Hillary Clinton',\n",
    "    'Climate Change is a Real Concern',\n",
    "    'Feminist Movement',\n",
    "    'Legalization of Abortion',\n",
    "    'Atheism'\n",
    "]\n",
    "\n",
    "models = {}\n",
    "\n",
    "for label in labels:\n",
    "    (x_train, y_train) = get_xy_data('trainingdata.txt', label, TENSED)\n",
    "    models[label] = train_model(label, x_train, y_train, TENSED)\n",
    "    \n",
    "\n",
    "(x_test, y_test) = get_xy_data('trialdata.txt', 'ALL', TENSED)\n",
    "(x_gold, y_gold) = get_xy_data('./Dans/subtaskA-testdata-gold.txt', 'ALL', TENSED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(rows, vec_data):\n",
    "    targets = list(map((lambda row: row['Target']), rows))\n",
    "    targets_and_data = zip(targets, vec_data)\n",
    "    predictions = []\n",
    "    for (target, data) in targets_and_data:\n",
    "        prediction = (models[target].predict(np_array([data])))[0]\n",
    "        predict_stance = STANCES[prediction.tolist().index(max(prediction))]\n",
    "        predictions.append(predict_stance)\n",
    "    return predictions\n",
    "\n",
    "all_rows = get_target_rows('./Dans/subtaskA-testdata-gold.txt', 'ALL')\n",
    "predictions = get_predictions(all_rows, x_gold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ID   Target                                              Tweet   Stance\n",
      "0  101  Atheism  dear lord thank u for all of ur blessings forg...  AGAINST\n",
      "1  102  Atheism  Blessed are the peacemakers, for they shall be...  AGAINST\n",
      "2  103  Atheism  I am not conformed to this world. I am transfo...  AGAINST\n",
      "3  104  Atheism  Salah should be prayed with #focus and #unders...  AGAINST\n",
      "4  105  Atheism  And stay in your houses and do not display you...  AGAINST\n"
     ]
    }
   ],
   "source": [
    "traindf = pandas.read_csv('./Dans/trainingData.txt', sep='\\t', encoding='latin1')\n",
    "print(traindf.head())\n",
    "testdf = pandas.read_csv('./Dans/subtaskA-testdata-gold.txt', sep='\\t', encoding='latin1')\n",
    "\n",
    "outdf = pandas.DataFrame(columns=['ID','Target','Tweet','Stance'])\n",
    "\n",
    "for topic in listOfTopics(traindf):\n",
    "    testExtract = testdf.loc[testdf['Target'] == topic]\n",
    "    testCorpus = list(map(lambda x: x[:-6], list(testExtract['Tweet'])))\n",
    "\n",
    "    ID = list(map(lambda x: str(x), list(testExtract['ID'])))\n",
    "    s = zip(ID, list(testExtract['Target']), testCorpus, list(predictions))\n",
    "\n",
    "    for x in s:\n",
    "        outdf.loc[len(outdf)] = list(x)\n",
    "\n",
    "\n",
    "outdf.set_index('ID')\n",
    "outdf.to_csv('output.txt', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

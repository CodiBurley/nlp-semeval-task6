{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "import collections\n",
    "\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "training_df = pandas.read_csv('trainingdata.txt', sep='\\t', encoding='latin1')\n",
    "STANCE = ['FAVOR', 'AGAINST', 'NONE']\n",
    "all_words = []\n",
    "\n",
    "for tweet in training_df['Tweet']:\n",
    "    all_words.extend(TweetTokenizer().tokenize(tweet))\n",
    "    \n",
    "\n",
    "def build_dataset(words, n_words):\n",
    "    \"\"\"Process raw inputs into a dataset.\"\"\"\n",
    "    count = [['UNK', -1]]\n",
    "    count.extend(collections.Counter(words).most_common(n_words - 1))\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "    data = list()\n",
    "    unk_count = 0\n",
    "    for word in words:\n",
    "        if word in dictionary:\n",
    "            index = dictionary[word]\n",
    "        else:\n",
    "            index = 0  # dictionary['UNK']\n",
    "            unk_count += 1\n",
    "        data.append(index)\n",
    "    count[0][1] = unk_count\n",
    "    reversed_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return data, count, dictionary, reversed_dictionary\n",
    "\n",
    "\n",
    "def words_to_numbers(dictionary, word_arr):\n",
    "    for i, word in enumerate(word_arr):\n",
    "        word_arr[i] = dictionary[word] if word in dictionary else 0\n",
    "    return word_arr\n",
    "\n",
    "\n",
    "def get_stance_to_num(stance):\n",
    "    return STANCE.index(stance)\n",
    "\n",
    "\n",
    "def create_training_data(training_data, num_top_words):\n",
    "    data, count, dictionary, reversed_dictionary = build_dataset(all_words, num_top_words)\n",
    "    X_data = []\n",
    "    Y_data = []\n",
    "    for _, row in training_data.iterrows():\n",
    "        X_data.append(words_to_numbers(dictionary, TweetTokenizer().tokenize(row['Tweet'])))\n",
    "        Y_data.append([get_stance_to_num(row['Stance'])])\n",
    "    return X_data, to_categorical(Y_data, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the training set\n",
    "\n",
    "Here we create the training set and pad it out to get everything prepped for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "\n",
    "num_top_words = 5000\n",
    "\n",
    "X_train, y_train = create_training_data(training_df, num_top_words)\n",
    "\n",
    "# Magic number that I computed so I can pad out the input arrays\n",
    "longest_row = 37\n",
    "# Lets pad those suckers out\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=longest_row, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jdeibel/anaconda/envs/nlp/lib/python3.5/site-packages/ipykernel_launcher.py:20: UserWarning: The `dropout` argument is no longer support in `Embedding`. You can apply a `keras.layers.SpatialDropout1D` layer right after the `Embedding` layer to get the same behavior.\n",
      "/Users/jdeibel/anaconda/envs/nlp/lib/python3.5/site-packages/ipykernel_launcher.py:21: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(196, recurrent_dropout=0.2, dropout=0.2)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_20 (Embedding)     (None, 37, 128)           640000    \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 196)               254800    \n",
      "_________________________________________________________________\n",
      "dense_55 (Dense)             (None, 3)                 591       \n",
      "=================================================================\n",
      "Total params: 895,391\n",
      "Trainable params: 895,391\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 1885 samples, validate on 929 samples\n",
      "Epoch 1/25\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "from keras import regularizers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "\n",
    "# create the model\n",
    "embed_dim = 128\n",
    "lstm_out = 196\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(num_top_words, embed_dim, input_length=longest_row, dropout=0.2))\n",
    "model.add(LSTM(lstm_out, dropout_U=0.2, dropout_W=0.2))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=25, validation_split=0.33, batch_size=16, verbose=1)\n",
    "\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# # summarize history for loss\n",
    "# plt.plot(history.history['loss'])\n",
    "# plt.plot(history.history['val_loss'])\n",
    "# plt.title('model loss')\n",
    "# plt.ylabel('loss')\n",
    "# plt.xlabel('epoch')\n",
    "# plt.legend(['train', 'test'], loc='upper left')\n",
    "# plt.savefig(\"thing.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ID   Target                                              Tweet   Stance\n",
      "0  101  Atheism  dear lord thank u for all of ur blessings forg...  AGAINST\n",
      "1  102  Atheism  Blessed are the peacemakers, for they shall be...  AGAINST\n",
      "2  103  Atheism  I am not conformed to this world. I am transfo...  AGAINST\n",
      "3  104  Atheism  Salah should be prayed with #focus and #unders...  AGAINST\n",
      "4  105  Atheism  And stay in your houses and do not display you...  AGAINST\n"
     ]
    }
   ],
   "source": [
    "# Evaluate F-score\n",
    "def listOfTopics(dataframe):\n",
    "    listOfTopics = []\n",
    "    for row in traindf['Target']:\n",
    "        if row not in listOfTopics:\n",
    "            listOfTopics.append(row)\n",
    "    return listOfTopics\n",
    "\n",
    "\n",
    "traindf = pandas.read_csv('./trainingData.txt', sep='\\t', encoding='latin1')\n",
    "print(traindf.head())\n",
    "testdf = pandas.read_csv('./subtaskA-testdata-gold.txt', sep='\\t', encoding='latin1')\n",
    "\n",
    "X_gold, _ = create_training_data(testdf, num_top_words)\n",
    "X_gold = sequence.pad_sequences(X_gold, maxlen=longest_row, padding='post')\n",
    "\n",
    "predictions = model.predict(X_gold)\n",
    "gold_predictions = [0] * len(predictions)\n",
    "\n",
    "for idx, prediction in enumerate(predictions):\n",
    "    gold_predictions[idx] = STANCE[prediction.tolist().index(max(prediction))]\n",
    "\n",
    "outdf = pandas.DataFrame(columns=['ID','Target','Tweet','Stance'])\n",
    "\n",
    "for topic in listOfTopics(traindf):\n",
    "    testExtract = testdf.loc[testdf['Target'] == topic]\n",
    "    testCorpus = list(map(lambda x: x[:-6], list(testExtract['Tweet'])))\n",
    "\n",
    "    ID = list(map(lambda x: str(x), list(testExtract['ID'])))\n",
    "    s = zip(ID, list(testExtract['Target']), testCorpus, list(gold_predictions))\n",
    "\n",
    "    for x in s:\n",
    "        outdf.loc[len(outdf)] = list(x)\n",
    "\n",
    "outdf.set_index('ID')\n",
    "outdf.to_csv('output.txt', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

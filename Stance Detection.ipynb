{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pickle\n",
    "import pandas\n",
    "\n",
    "from nltk.parse.stanford import StanfordParser\n",
    "\n",
    "def save_object(arr, filename):\n",
    "    with open(filename, 'wb') as output:\n",
    "        pickle.dump(arr, output, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# Change the path according to your system\n",
    "stanford_classifier = '/Users/jdeibel/dev/school/nlp/libraries/classifiers/english.all.3class.distsim.crf.ser.gz'\n",
    "stanford_ner_path = '/Users/jdeibel/dev/school/nlp/libraries/stanford-ner.jar'\n",
    "\n",
    "stanford_parser_dir = '/Users/jdeibel/dev/school/nlp/libraries/'\n",
    "eng_model_path = stanford_parser_dir + \"edu/stanford/nlp/models/lexparser/englishRNN.ser.gz\"\n",
    "my_path_to_models_jar = stanford_parser_dir + \"models/stanford-english-corenlp-2017-06-09-models.jar\"\n",
    "my_path_to_jar = stanford_parser_dir + \"stanford-parser.jar\"\n",
    "\n",
    "parser = StanfordParser(model_path=eng_model_path, path_to_models_jar=my_path_to_models_jar, path_to_jar=my_path_to_jar)\n",
    "\n",
    "ROOT = 'ROOT'\n",
    "constituentDictionary = None\n",
    "globalConstDict = {}\n",
    "trainData = []\n",
    "\n",
    "\n",
    "def add_to_train(constituents, tweet, stance, target):\n",
    "    trainData.append({\n",
    "        'const': constituents, \n",
    "        'tweet': tweet, \n",
    "        'stance': stance, \n",
    "        'target': target\n",
    "    })\n",
    "    save_object(trainData, \"train_0.txt\")\n",
    "    \n",
    "\n",
    "def get_nodes(parent, tweet, stance, target):\n",
    "    for node in parent:\n",
    "        if type(node) is nltk.Tree:\n",
    "            if node.label() != ROOT:\n",
    "                if node.label() not in constituentDictionary:\n",
    "                    constituentDictionary[node.label()] = 1\n",
    "                else:\n",
    "                    constituentDictionary[node.label()] = constituentDictionary[node.label()] + 1\n",
    "                # Keep global track of the constituents\n",
    "                if node.label() not in globalConstDict:\n",
    "                    globalConstDict[node.label()] = 1\n",
    "                else:\n",
    "                    globalConstDict[node.label()] = globalConstDict[node.label()] + 1\n",
    "                    \n",
    "            get_nodes(node, tweet, stance, target)\n",
    "\n",
    "\n",
    "def create_training_data():\n",
    "    df = pandas.read_csv('trainingdata.txt', sep='\\t', encoding=\"latin1\")\n",
    "    \n",
    "    itr = 0\n",
    "    for item in df.iterrows():\n",
    "        index, row = item\n",
    "        parsedTree = list(parser.raw_parse(row.Tweet))\n",
    "        for tree in parsedTree:\n",
    "            constituentDictionary = {}\n",
    "            get_nodes(tree, row.Tweet, row.Stance, row.Target)\n",
    "            add_to_train(constituentDictionary, row.Tweet, row.Stance, row.Target)\n",
    "            sys.stdout.write(\"\\r\" + str(itr) + \" \" + str(globalConstDict))\n",
    "            sys.stdout.flush()\n",
    "            itr += 1\n",
    "            \n",
    "    save_object(globalConstDict, \"total_constituents.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "parsedConstituents = None\n",
    "\n",
    "with open(\"train_0.txt\", \"rb\") as f:\n",
    "    parsedConstituents = pickle.load(f)\n",
    "import nltk\n",
    "\n",
    "Constituents = {'VB': 2871, 'PRN': 165, 'PRP': 2912, 'PP': 3802, \n",
    "         'QP': 75, 'RBR': 69, 'DT': 3283, 'RRC': 25, 'VBP': 1963, \n",
    "         'WRB': 384, 'PDT': 38, 'WHNP': 481, 'WP': 314, 'CD': 763, \n",
    "         'SBARQ': 154, \"''\": 282, 'MD': 771, 'EX': 77, 'NAC': 27, \n",
    "         'RP': 180, 'SBAR': 3005, 'CONJP': 8, 'POS': 246, 'FRAG': 1401, \n",
    "         '$': 30, 'NNP': 4036, 'VBZ': 1801, 'VP': 10395, 'ADJP': 2027, \n",
    "         '-RRB-': 76, '``': 266, 'FW': 75, 'WHADVP': 352, 'UCP': 107, \n",
    "         'NN': 9801, 'RBS': 39, 'CC': 1073, 'JJR': 119, 'RB': 2776, \n",
    "         'X': 854, '-LRB-': 73, 'UH': 82, 'SQ': 324, 'TO': 1187, \n",
    "         'JJ': 4962, 'IN': 4180, 'NP': 19774, 'INTJ': 100, 'S': 6872, \n",
    "         'VBG': 964, 'NX': 3, ':': 761, 'VBN': 734, 'JJS': 95, '#': 34, \n",
    "         'WHADJP': 22, 'LST': 2, 'ADVP': 1504, 'WHPP': 9, 'SYM': 78, \n",
    "         'WDT': 129, 'WP$': 4, '.': 2951, 'NNS': 2695, ',': 1140, \n",
    "         'VBD': 634, 'SINV': 279, 'PRT': 178, 'LS': 2, 'PRP$': 798, \n",
    "         'NNPS': 70}\n",
    "\n",
    "TENSED_POS_TAGS = ['POS_VBD', 'POS_VBG', 'POS_VBN', 'POS_VBP', 'POS_VBZ']\n",
    "XKey = []\n",
    "YKey = ['AGAINST', 'FAVOR', 'NONE']\n",
    "\n",
    "# Append the types of constituents and POS tags into our master key array to be used to each\n",
    "# input row. Each index corresponds to that feature's count and position in our input row of\n",
    "# our input tensor\n",
    "for const_key in Constituents:\n",
    "    XKey.append(const_key)\n",
    "for pos_key in TENSED_POS_TAGS:\n",
    "    XKey.append(pos_key)\n",
    "\n",
    "\n",
    "def get_key_index(key):\n",
    "    return XKey.index(key)\n",
    "\n",
    "\n",
    "def constituents_to_features(feature_arr, consts):\n",
    "    for constituent in consts:\n",
    "        feature_arr[get_key_index(constituent)] = consts[constituent]\n",
    "    return feature_arr\n",
    "\n",
    "\n",
    "def pos_tags_to_features(feature_arr, tweet):\n",
    "    pos_tags = nltk.pos_tag(nltk.word_tokenize(tweet))\n",
    "    for tag in pos_tags[1]:\n",
    "        modified_tag = 'POS_' + tag\n",
    "        if modified_tag in XKey:\n",
    "            feature_arr[get_key_index('POS_' + tag)] += 5\n",
    "\n",
    "\n",
    "def build_features(constituents, tweet):\n",
    "    feature_arr = []\n",
    "    for i in range(0, len(XKey)):\n",
    "        feature_arr.append(0)\n",
    "    constituents_to_features(feature_arr, constituents)\n",
    "    pos_tags_to_features(feature_arr, tweet)\n",
    "    return feature_arr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "normal_feature_array = []\n",
    "normal_label_array = []\n",
    "\n",
    "for item in parsedConstituents:\n",
    "    normal_feature_array.append(build_features(item[\"const\"], item[\"tweet\"]))\n",
    "    normal_label_array.append(YKey.index(item[\"stance\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "# To split data stuff\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from keras import regularizers\n",
    "\n",
    "OUTPUT = 3\n",
    "\n",
    "np_feat_arr = np.array(normal_feature_array)\n",
    "np_label_arr = to_categorical(normal_label_array, OUTPUT)\n",
    "\n",
    "# Do a split\n",
    "X_train, X_test, y_train, y_test = train_test_split(np_feat_arr, \n",
    "                                                    np_label_arr, \n",
    "                                                    test_size=0.02, \n",
    "                                                    random_state=46)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(units=len(XKey), \n",
    "                input_dim=len(XKey),\n",
    "                activity_regularizer=regularizers.l1(0.01)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(units=16))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(units=64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(units=12))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(units=OUTPUT))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adam(lr=0.00001),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Fit the model\n",
    "history = model.fit(X_train, y_train, validation_split=0.33, \n",
    "                    epochs=50, batch_size=16, verbose=1)\n",
    "# list all data in history\n",
    "print(history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "loss_and_metrics = model.evaluate(X_test, y_test, batch_size=32)\n",
    "\n",
    "print(loss_and_metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
